{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the `Last.fm` Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import sparse as sp\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import check_consistent_length\n",
    "\n",
    "SEED = 1\n",
    "DATA_DIR = (\n",
    "    Path().cwd().parent.joinpath(\"data\", \"processed\", \"lastfm\", f\"seed_{SEED}\")\n",
    ")\n",
    "plt.style.use('seaborn-poster')\n",
    "mpl.rcParams['figure.autolayout'] = True\n",
    "assert DATA_DIR.is_dir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "Edges between users are undirected, and `edge_uu.txt` only stores the indices of the upper triangular part of the adjacency matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ratings`: a 2d `numpy.ndarray` object.\n",
    "# Each row is a `[uid, iid, label]` triplet.\n",
    "ratings = np.unique(\n",
    "    np.loadtxt(DATA_DIR.joinpath(\"ratings.txt\"), dtype=np.int64), axis=0\n",
    ")\n",
    "# `triplets_kg`: a 2d `numpy.ndarray` object.\n",
    "# Each row is a `[eid_h, rid, eid_t]` triplet.\n",
    "triplets_kg = np.unique(\n",
    "    np.loadtxt(DATA_DIR.joinpath(\"triplets_kg.txt\"), dtype=np.int64), axis=0\n",
    ")\n",
    "# `edges_user`: a 2d `numpy.ndarray` object.\n",
    "# Each row is an unordered `[uid_u, uid_v]` pair.\n",
    "edges_user = np.unique(\n",
    "    np.loadtxt(DATA_DIR.joinpath(\"edges_uu.txt\"), dtype=np.int64), axis=0\n",
    ")\n",
    "assert ratings.ndim == 2 and ratings.shape[1] == 3\n",
    "assert triplets_kg.ndim == 2 and triplets_kg.shape[1] == 3\n",
    "assert edges_user.ndim == 2 and edges_user.shape[1] == 2\n",
    "# indices of the upper triangular part of the adjacency matrix\n",
    "assert np.all(edges_user[:, 0] < edges_user[:, 1])\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            f\"num_ratings = {ratings.shape[0]}\",\n",
    "            f\"num_triplets = {triplets_kg.shape[0]}\",\n",
    "            f\"num_edges_user = {edges_user.shape[0]}\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = ratings[:, 0].max() + 1\n",
    "num_items = ratings[:, 1].max() + 1\n",
    "num_entities = triplets_kg[:, [0, 2]].max() + 1\n",
    "num_relations = triplets_kg[:, 1].max() + 1\n",
    "assert num_items < num_entities\n",
    "assert edges_user.max() < num_users\n",
    "sparsity_ui = ratings.shape[0] / num_users / num_items\n",
    "sparsity_uu = edges_user.shape[0] * 2 / num_users / (num_users - 1)\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            f\"num_users = {num_users}\",\n",
    "            f\"num_items = {num_items}\",\n",
    "            f\"num_entities = {num_entities}\",\n",
    "            f\"num_relations = {num_relations}\",\n",
    "            f\"sparsity_ui = {sparsity_ui}\",\n",
    "            f\"sparsity_uu = {sparsity_uu}\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Item Interaction Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes user history to a vector\n",
    "# `user_history` is a `nnumpy.ndarray` object of shape `[num_users, num_items]`\n",
    "# For each positive sample `(uid, iid)`, `user_history[uid, iid] = 1`.\n",
    "ratings_pos = ratings[ratings[:, 2] == 1]\n",
    "user_history = sp.csr_matrix(\n",
    "    ([1.0] * ratings_pos.shape[0], (ratings_pos[:, 0], ratings_pos[:, 1])),\n",
    "    shape=(num_users, num_items),\n",
    "    dtype=np.float32,\n",
    ")\n",
    "user_history.nnz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_u = user_history.sum(axis=1).A.flatten()\n",
    "deg_i = user_history.sum(axis=0).A.flatten()\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            f\"deg_u: mean = {np.mean(deg_u)}, std = {np.std(deg_u)}\",\n",
    "            f\"deg_i: mean = {np.mean(deg_i)}, std = {np.std(deg_i)}\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(deg_u, return_counts=True), np.unique(deg_i, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "adj_list_kg = defaultdict(list)\n",
    "for eid_h, rid, eid_t in triplets_kg:\n",
    "    assert eid_h < num_items\n",
    "    if eid_t < num_items:\n",
    "        cnt += 1\n",
    "    adj_list_kg[eid_h].append((rid, eid_t))\n",
    "deg_i_kg = np.asarray([len(adj_list_kg[iid]) for iid in range(num_items)])\n",
    "cnt, np.unique(deg_i_kg, return_counts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity between Users Connected by Social Edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Common Neighbors & Jaccard Measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors_jaccard(\n",
    "    y_true: sp.spmatrix, y_pred: sp.spmatrix\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    assert y_true.ndim == 2 and y_pred.ndim == 2\n",
    "    check_consistent_length(y_true, y_pred)\n",
    "    y_true = y_true.astype(np.bool_).astype(np.int8)\n",
    "    y_pred = y_pred.astype(np.bool_).astype(np.int8)\n",
    "    union = y_true.multiply(y_pred)\n",
    "    intersection = (y_true + y_pred).astype(np.bool_).astype(np.int8)\n",
    "    num_union = union.sum(axis=1).A.astype(np.float32)\n",
    "    num_intersection = intersection.sum(axis=1).A.astype(np.float32)\n",
    "    return num_union, num_union / num_intersection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `common_nbrs_pos`: the number of common neighbors between users\n",
    "# connected by edges\n",
    "# `jaccard_pos`: the jaccard measure between users connected by edges\n",
    "common_nbrs_pos, jaccard_pos = common_neighbors_jaccard(\n",
    "    user_history[edges_user[:, 0], :], user_history[edges_user[:, 1], :]\n",
    ")\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            f\"common_nbrs_pos: mean = {np.mean(common_nbrs_pos)}, \"\n",
    "            f\"std = {np.std(common_nbrs_pos)}, \"\n",
    "            f\"median = {np.median(common_nbrs_pos)}\",\n",
    "            f\"jaccard_pos: mean = {np.mean(jaccard_pos)}, \"\n",
    "            f\"std = {np.std(jaccard_pos)}, \"\n",
    "            f\"median = {np.median(jaccard_pos)}\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the Last.fm dataset, edges are undirected.\n",
    "# The number of possible edges is N = `(num_users - 1) * num_users / 2``\n",
    "def encode_indices_batch(rows: np.ndarray, cols: np.ndarray) -> np.ndarray:\n",
    "    # converts a `(row, col)` pair to [0, N - 1]\n",
    "    assert np.all(rows < cols)\n",
    "    return rows + cols * (cols - 1) // 2\n",
    "\n",
    "\n",
    "def decode_indices_batch(\n",
    "    indices: np.ndarray, size: int\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # converts an integer in the range [0, N - 1] to a `(row, col)` pair\n",
    "    bins = np.cumsum(np.arange(size))\n",
    "    cols = np.digitize(indices, bins, right=False)\n",
    "    rows = indices - cols * (cols - 1) // 2\n",
    "    return rows, cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_pos = encode_indices_batch(edges_user[:, 0], edges_user[:, 1])\n",
    "assert np.unique(indices_pos).size == indices_pos.size\n",
    "indices_neg = np.arange((num_users) * (num_users - 1) // 2, dtype=np.int64)\n",
    "indices_neg = indices_neg[np.isin(indices_neg, indices_pos, invert=True)]\n",
    "assert np.unique(indices_neg).size == indices_neg.size\n",
    "rows, cols = decode_indices_batch(indices_neg, size=num_users)\n",
    "assert np.all(rows < cols)\n",
    "f\"num_edges_user_neg = {rows.size}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `common_nbrs_neg`: the number of common neighbors between users\n",
    "# that are not connected\n",
    "# `jaccard_neg`: the jaccard measure between users that are not connected\n",
    "common_nbrs_neg, jaccard_neg = common_neighbors_jaccard(\n",
    "    user_history[rows, :], user_history[cols, :]\n",
    ")\n",
    "print(\n",
    "    \"\\n\".join(\n",
    "        [\n",
    "            f\"common_nbrs_neg: mean = {np.mean(common_nbrs_neg)}, \"\n",
    "            f\"std = {np.std(common_nbrs_neg)}, \"\n",
    "            f\"median = {np.median(common_nbrs_neg)}\",\n",
    "            f\"jaccard_neg: mean = {np.mean(jaccard_neg)}, \"\n",
    "            f\"std = {np.std(jaccard_neg)}, \"\n",
    "            f\"median = {np.median(jaccard_neg)}\",\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_max, v_min = int(common_nbrs_pos.max()), int(common_nbrs_pos.min())\n",
    "\n",
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111)\n",
    "hist, bins, _ = ax.hist(\n",
    "    common_nbrs_pos, bins=np.arange(v_min, v_max + 1), density=True\n",
    ")\n",
    "hist.sum(), hist, bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_max, v_min = int(common_nbrs_neg.max()), int(common_nbrs_pos.min())\n",
    "\n",
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111)\n",
    "hist, bins, _ = ax.hist(\n",
    "    common_nbrs_neg, bins=np.arange(v_min, v_max + 1), density=True\n",
    ")\n",
    "hist.sum(), hist, bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matthews Correlation Coefficient for Each Item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_per_item = np.zeros((num_items,), dtype=np.float32)\n",
    "for iid in range(num_items):\n",
    "    y_u = (\n",
    "        user_history[edges_user[:, 0], iid].astype(np.bool_).toarray().flatten()\n",
    "    )\n",
    "    y_v = (\n",
    "        user_history[edges_user[:, 1], iid].astype(np.bool_).toarray().flatten()\n",
    "    )\n",
    "    mcc_per_item[iid] = matthews_corrcoef(y_u, y_v)\n",
    "mcc_per_item.max(), mcc_per_item.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = deg_i >= 5\n",
    "mcc_per_item_valid = mcc_per_item[mask]\n",
    "mcc_per_item_valid.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111)\n",
    "hist, bins, _ = ax.hist(\n",
    "    mcc_per_item, bins=np.linspace(-0.1, 1, num=12), density=False\n",
    ")\n",
    "hist.sum(), hist, bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure()\n",
    "ax = figure.add_subplot(111)\n",
    "hist, bins, _ = ax.hist(\n",
    "    mcc_per_item_valid, bins=np.linspace(-0.1, 1, num=12), density=False\n",
    ")\n",
    "hist.sum(), hist, bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block is computationally expensive.\n",
    "# indices_pos = encode_indices_batch(edges_user[:, 0], edges_user[:, 1])\n",
    "# assert np.unique(indices_pos).size == indices_pos.size\n",
    "# indices_neg = np.arange((num_users) * (num_users - 1) // 2, dtype=np.int64)\n",
    "# indices_neg = indices_neg[np.isin(indices_neg, indices_pos, invert=True)]\n",
    "# assert np.unique(indices_neg).size == indices_neg.size\n",
    "# rows, cols = decode_indices_batch(indices_neg, size=num_users)\n",
    "# assert np.all(rows < cols)\n",
    "\n",
    "\n",
    "# mcc_per_item = np.zeros((num_items,), dtype=np.float32)\n",
    "# for iid in range(num_items):\n",
    "#     if (iid + 1) % 300 == 0:\n",
    "#         print(iid)\n",
    "#     y_u = (\n",
    "#         user_history[rows, iid].astype(np.bool_).toarray().flatten()\n",
    "#     )\n",
    "#     y_v = (\n",
    "#         user_history[cols, iid].astype(np.bool_).toarray().flatten()\n",
    "#     )\n",
    "#     mcc_per_item[iid] = matthews_corrcoef(y_u, y_v)\n",
    "# mcc_per_item.max(), mcc_per_item.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure = plt.figure()\n",
    "# ax = figure.add_subplot(111)\n",
    "# hist, bins, _ = ax.hist(\n",
    "#     mcc_per_item, bins=np.linspace(-0.1, 1, num=12), density=False\n",
    "# )\n",
    "# hist.sum(), hist, bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters of Each Item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_user = nx.Graph()\n",
    "graph_user.add_edges_from(edges_user)\n",
    "(\n",
    "    f\"#nodes = {graph_user.number_of_nodes()}, \"\n",
    "    f\"#edges = {graph_user.number_of_edges()}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids_valid = set()\n",
    "for comp in nx.connected_components(graph_user):\n",
    "    if len(comp) > len(uids_valid):\n",
    "        uids_valid = comp\n",
    "uids_valid = np.asarray(sorted(uids_valid))\n",
    "f\"#nodes = {uids_valid.size}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `(src, tgt)` -> length of the shortest path between `src` and `dst`\n",
    "sp_len_cache = {}\n",
    "\n",
    "\n",
    "def average_shortest_path_length(graph: nx.Graph, nodes: np.ndarray) -> float:\n",
    "    dists = []\n",
    "    for i in range(nodes.size):\n",
    "        src = nodes[i]\n",
    "        for j in range(i + 1, nodes.size):\n",
    "            tgt = nodes[j]\n",
    "            if (src, tgt) not in sp_len_cache:\n",
    "                sp_len_cache[(src, tgt)] = nx.shortest_path_length(\n",
    "                    graph, src, tgt\n",
    "                )\n",
    "            dists.append(sp_len_cache[(src, tgt)])\n",
    "    return np.mean(dists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_dist_per_item = np.zeros((num_items,), dtype=np.float32)\n",
    "deg_i_valid = np.zeros((num_items,), dtype=np.float32)\n",
    "\n",
    "\n",
    "for iid in range(num_items):\n",
    "    dists = []\n",
    "    uids = user_history[:, iid].nonzero()[0]\n",
    "    uids = uids[np.isin(uids, uids_valid)]\n",
    "    deg_i_valid[iid] = uids.size\n",
    "    if uids.size < 2:\n",
    "        avg_dist_per_item[iid] = -1\n",
    "        continue\n",
    "    avg_dist_per_item[iid] = average_shortest_path_length(graph_user, uids)\n",
    "avg_dist_per_item.max(), avg_dist_per_item.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `deg` -> average length of shortest paths between randomly selected nodes\n",
    "avg_dist_per_deg = {}\n",
    "avg_dist_std_per_deg = {}\n",
    "num_runs = 30\n",
    "for deg in np.unique(deg_i_valid):\n",
    "    deg = int(deg)\n",
    "    if deg < 2:\n",
    "        avg_dist = -1.0\n",
    "        avg_dist_std = 0.0\n",
    "    else:\n",
    "        avg_dist_per_run = []\n",
    "        for _ in range(num_runs):\n",
    "            uids_rand = np.random.choice(uids_valid, size=deg, replace=False)\n",
    "            avg_dist_per_run.append(\n",
    "                average_shortest_path_length(graph_user, uids_rand)\n",
    "            )\n",
    "        avg_dist = np.mean(avg_dist_per_run)\n",
    "        avg_dist_std = np.std(avg_dist_per_run)\n",
    "    avg_dist_per_deg[deg] = avg_dist\n",
    "    avg_dist_std_per_deg[deg] = avg_dist_std\n",
    "std_values = list(avg_dist_std_per_deg.values())\n",
    "np.mean(std_values), np.max(std_values), np.min(std_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iid in range(num_items):\n",
    "    deg = deg_i_valid[iid]\n",
    "    print(\n",
    "        \"\\t\".join(\n",
    "            [\n",
    "                f\"{iid}\",\n",
    "                f\"{deg}\",\n",
    "                f\"{avg_dist_per_item[iid]}\",\n",
    "                f\"{avg_dist_per_deg[deg]}\",\n",
    "                f\"{avg_dist_std_per_deg[deg]}\",\n",
    "            ]\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_per_iid = {}\n",
    "# for iid in range(num_items):\n",
    "#     uids = user_history[:, iid].nonzero()\n",
    "#     assert np.all(uids[1] == 0)\n",
    "#     uids = uids[0]\n",
    "#     rows, cols = [], []\n",
    "#     for i in range(0, uids.size):\n",
    "#         for j in range(i + 1, uids.size):\n",
    "#             rows.append(uids[i])\n",
    "#             rows.append(uids[j])\n",
    "#             cols.append(uids[j])\n",
    "#             cols.append(uids[i])\n",
    "#     adj_per_iid.append(\n",
    "#         sp.csr_matrix(\n",
    "#             ([1.0] * len(rows), (rows, cols)),\n",
    "#             shape=(num_users, num_users),\n",
    "#             dtype=np.float32,\n",
    "#         )\n",
    "#     )\n",
    "# len(adj_per_iid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iid in range(num_items):\n",
    "#     assert adj_per_iid[iid].nnz == deg_i[iid] * (deg_i[iid] - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_linear(X):\n",
    "    return X @ X.T\n",
    "\n",
    "\n",
    "def gram_rbf(X, threshold=1.0):\n",
    "    gram = gram_linear(X)\n",
    "    norms = torch.diag(gram)\n",
    "    dist = -2 * gram + norms[:, None] + norms[None, :]\n",
    "    dist_median = torch.median(dist).clamp_min_(torch.finfo(torch.float).tiny)\n",
    "    rbf = torch.exp(-dist / (2 * threshold ** 2 * dist_median))\n",
    "    return rbf\n",
    "\n",
    "\n",
    "def center_gram(gram):\n",
    "    means = torch.mean(gram, dim=0)\n",
    "    means -= torch.mean(means) / 2\n",
    "    gram -= means[:, None]\n",
    "    gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "def cka(X, Y, mode=\"linear\", threshold=1.0):\n",
    "    if mode == \"linear\":\n",
    "        gram_X = gram_linear(X)\n",
    "        gram_Y = gram_linear(Y)\n",
    "    elif mode == \"rbf\":\n",
    "        gram_X = gram_rbf(X, threshold)\n",
    "        gram_Y = gram_rbf(Y, threshold)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown mode {}\".format(mode))\n",
    "\n",
    "    gram_X = center_gram(gram_X)\n",
    "    gram_Y = center_gram(gram_Y)\n",
    "    scaled_hsic = gram_X.ravel() @ gram_Y.ravel()\n",
    "    norm_X = torch.linalg.norm(gram_X)\n",
    "    norm_Y = torch.linalg.norm(gram_Y)\n",
    "    rst = scaled_hsic / (norm_X * norm_Y)\n",
    "\n",
    "    return rst\n",
    "\n",
    "\n",
    "def cca(X, Y):\n",
    "    Qx, _ = torch.linalg.qr(X)\n",
    "    Qy, _ = torch.linalg.qr(Y)\n",
    "    rst = torch.linalg.norm(Qx.T @ Qy) ** 2 / min(X.shape[1], Y.shape[1])\n",
    "\n",
    "    return rst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cca_per_iid = []\n",
    "# for iid in range(num_items):\n",
    "#     cca_per_iid.append(\n",
    "#         cca(\n",
    "#             torch.as_tensor(adj_mat.toarray(), dtype=torch.float),\n",
    "#             torch.as_tensor(adj_per_iid[iid].toarray(), dtype=torch.float),\n",
    "#         )\n",
    "#     )\n",
    "# cca_per_iid = np.asarray([v.item() for v in cca_per_iid], dtype=np.float32)\n",
    "# cca_per_iid.shape, np.mean(cca_per_iid), np.std(cca_per_iid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deg_i, cca_per_iid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CKA (Linear)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iid_cka_linear = {}\n",
    "# for iid in range(num_items):\n",
    "#     if deg_i[iid] == 1:\n",
    "#         continue\n",
    "#     iid_cka_linear[iid] = cka(\n",
    "#         torch.as_tensor(adj_mat.toarray(), dtype=torch.float),\n",
    "#         torch.as_tensor(adj_per_iid[iid].toarray(), dtype=torch.float),\n",
    "#         mode=\"linear\",\n",
    "#     ).item()\n",
    "# iid_cka_linear = sorted(\n",
    "#     iid_cka_linear.items(), key=lambda x_y: x_y[1], reverse=True\n",
    "# )\n",
    "# iid_cka_linear[:10], iid_cka_linear[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (iid, cka_coef) in iid_cka_linear:\n",
    "#     print(f\"{iid}\\t{deg_i[iid]}\\t{cka_coef}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CKA (RBF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 1.0\n",
    "# iid_cka_rbf = {}\n",
    "# for iid in range(num_items):\n",
    "#     if deg_i[iid] == 1:\n",
    "#         continue\n",
    "#     iid_cka_rbf[iid] = cka(\n",
    "#         torch.as_tensor(adj_mat.toarray(), dtype=torch.float),\n",
    "#         torch.as_tensor(adj_per_iid[iid].toarray(), dtype=torch.float),\n",
    "#         mode=\"rbf\",\n",
    "#         threshold=threshold,\n",
    "#     ).item()\n",
    "# iid_cka_rbf = sorted(iid_cka_rbf.items(), key=lambda x_y: x_y[1], reverse=True)\n",
    "# iid_cka_rbf[:10], iid_cka_rbf[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0.1\n",
    "# iid_cka_rbf = {}\n",
    "# for iid in range(num_items):\n",
    "#     if deg_i[iid] == 1:\n",
    "#         continue\n",
    "#     iid_cka_rbf[iid] = cka(\n",
    "#         torch.as_tensor(adj_mat.toarray(), dtype=torch.float),\n",
    "#         torch.as_tensor(adj_per_iid[iid].toarray(), dtype=torch.float),\n",
    "#         mode=\"rbf\",\n",
    "#         threshold=threshold,\n",
    "#     ).item()\n",
    "# iid_cka_rbf = sorted(iid_cka_rbf.items(), key=lambda x_y: x_y[1], reverse=True)\n",
    "# iid_cka_rbf[:10], iid_cka_rbf[-10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 3.0\n",
    "# iid_cka_rbf = {}\n",
    "# for iid in range(num_items):\n",
    "#     if deg_i[iid] == 1:\n",
    "#         continue\n",
    "#     iid_cka_rbf[iid] = cka(\n",
    "#         torch.as_tensor(adj_mat.toarray(), dtype=torch.float),\n",
    "#         torch.as_tensor(adj_per_iid[iid].toarray(), dtype=torch.float),\n",
    "#         mode=\"rbf\",\n",
    "#         threshold=threshold,\n",
    "#     ).item()\n",
    "# iid_cka_rbf = sorted(iid_cka_rbf.items(), key=lambda x_y: x_y[1], reverse=True)\n",
    "# iid_cka_rbf[:10], iid_cka_rbf[-10:]\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98215ffbdaf5e807ce7896eb846bcd2d1ce71c3892f0b2e8a92fbe7dc26fa398"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('pytorch_dgl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
